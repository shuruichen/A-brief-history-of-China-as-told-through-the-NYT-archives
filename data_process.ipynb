{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87970ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "\n",
    "with open('/Users/mac/Desktop/python_project/scraped_data/1960-1969.txt','r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        i += 1\n",
    "        try:\n",
    "            dic = json.loads(line,strict = False)\n",
    "            docs = dic['response']['docs']\n",
    "            for doc in docs:\n",
    "                l1.append(doc[\"web_url\"])\n",
    "                l2.append(doc[\"pub_date\"])\n",
    "                l3.append(doc[\"headline\"][\"main\"])\n",
    "        except Exception as e:\n",
    "            print(e) #print lines containing damaged data\n",
    "            continue\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c751faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove NoneType in l3\n",
    "\n",
    "def not_empty(s):\n",
    "    return s and s.strip()\n",
    "l3_remove = list(filter(not_empty,l3))\n",
    "\n",
    "\n",
    "\n",
    "#remove punctuations\n",
    "\n",
    "punctuation = ['(', ')', '?', ':', ';', ',', '.', '!', '/', '\"', \"'\",\"-\",\"%\",\"$\",\"&\",\"*\",\"+\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "l4 = []\n",
    "\n",
    "for line in l3_remove:\n",
    "    newline = ''\n",
    "    for c in line:\n",
    "        if c not in punctuation:\n",
    "            newline = newline + c\n",
    "    l4.append(newline)\n",
    "\n",
    "#case conversion\n",
    "l4_low = list(map(lambda x: x.lower(), l4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecf89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization, Stemming, Lemmatization\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "l4_tag = []\n",
    "for headline in l4_low:\n",
    "    token = word_tokenize(headline)  \n",
    "    tagged_sent = pos_tag(token)     \n",
    "    l4_tag.append(tagged_sent)\n",
    "    \n",
    "wnl = WordNetLemmatizer()\n",
    "l4_lemmas = []\n",
    "\n",
    "for tag in l4_tag:\n",
    "    for t in tag:\n",
    "        wordnet_pos = get_wordnet_pos(t[1]) or wordnet.NOUN\n",
    "        l4_lemmas.append(wnl.lemmatize(t[0], pos=wordnet_pos)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487895f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "l4_stop = []\n",
    "for word in l4_lemmas: \n",
    "    if word not in en_stops:\n",
    "        l4_stop.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d94c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get high-frequency words\n",
    "\n",
    "l6 = []\n",
    "for line in l4_stop:\n",
    "    newline = line.split(' ')\n",
    "    l6.extend(newline)\n",
    "\n",
    "wordcount = {}\n",
    "for word in l6:\n",
    "    if word in wordcount:\n",
    "        wordcount[word] += 1\n",
    "    else:\n",
    "        wordcount[word] = 1\n",
    "\n",
    "sorted(wordcount.items(),key=lambda item:item[1],reverse = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the mentioning possibility of each high-frequency word\n",
    "\n",
    "l_all=sorted(wordcount.items(),key=lambda item:item[1],reverse = True)\n",
    "\n",
    "words_all = []\n",
    "\n",
    "for item in l_all:\n",
    "    if not item[0] in words_all:\n",
    "        words_all.append(item[0])\n",
    "\n",
    "print(words_all)  \n",
    "\n",
    "word_dic = {}\n",
    "\n",
    "for word in words_all:\n",
    "    word_dic[word] = l3.count(word)\n",
    "\n",
    "\n",
    "word_dic_sorted = dict(sorted(word_dic.items(), key=lambda i:i[0]))\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "word_df = pd.DataFrame.from_dict(word_dic_sorted, orient='index')\n",
    " \n",
    "word_df.to_excel('/Users/mac/Desktop/1960.xlsx',sheet_name='Sheet1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
